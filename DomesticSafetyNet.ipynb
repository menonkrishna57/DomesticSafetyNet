{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menonkrishna57/DomesticSafetyNet/blob/main/DomesticSafetyNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW9CmXexgi1n"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow transformers pandas scikit-learn spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCrpLu1DQ6Lp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import spacy\n",
        "\n",
        "print(\"Loading spaCy model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model 'en_core_web_sm' loaded successfully.\")\n",
        "except IOError:\n",
        "    print(\"Error loading 'en_core_web_sm'. Please ensure it's installed.\")\n",
        "\n",
        "\n",
        "def anonymize_text(text):\n",
        "    text = re.sub(r'(\\+91[\\-\\s]?)?[0]?[789]\\d{9}', '[PHONE]', text)\n",
        "    text = re.sub(r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b', '[AADHAAR]', text)\n",
        "    text = re.sub(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})', '[PHONE]', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
        "    text = re.sub(r'[A-Z]{2}[0-9]{1,2}[A-Z]{1,2}[0-9]{1,4}', '[LICENSE_PLATE]', text)\n",
        "    text = re.sub(r'\\b\\d{1,5}\\s[\\w\\s,.-]+(?:Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Lane|Ln|Court|Ct|Colony|Nagar|Vihar)\\b', '[ADDRESS]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b(?:Apartment|Apt\\.?|Flat|Flt\\.|House No\\.?|H\\.?No\\.?|C-|B-)\\s?[\\w\\s\\d-]+', '[ADDRESS]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[A-Z]{5}[0-9]{4}[A-Z]{1}', '[PAN]', text)\n",
        "\n",
        "    if 'nlp' in globals():\n",
        "        doc = nlp(text)\n",
        "        new_text = text\n",
        "        for ent in reversed(doc.ents):\n",
        "            if ent.label_ in [\"PERSON\", \"GPE\", \"LOC\", \"ORG\"]:\n",
        "                new_text = new_text[:ent.start_char] + f\"[{ent.label_}]\" + new_text[ent.end_char:]\n",
        "        text = new_text\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def anonymize_and_clean_for_bert(text):\n",
        "    anonymized_text = anonymize_text(text)\n",
        "\n",
        "    cleaned_text = anonymized_text.lower()\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECtvKSTtVkUy",
        "outputId": "e703f813-9871-47d6-ada6-04e5238af794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from ./dataset/synthetic_data_1000.json...\n",
            "Successfully loaded 999 entries.\n",
            "\n",
            "Anonymizing and cleaning text data (using advanced NER method)...\n",
            "\n",
            "Label mapping created: {'High-Urgency': 0, 'Medium': 1, 'Immediate-Threat': 2, 'Low': 3}\n",
            "\n",
            "Data split into 799 training samples and 200 testing samples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3934098282.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['label_id'] = df['risk_level'].replace(label_dict)\n"
          ]
        }
      ],
      "source": [
        "file_path = './dataset/synthetic_data_1000.json'\n",
        "\n",
        "print(f\"Loading data from {file_path}...\")\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"Successfully loaded {len(df)} entries.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {file_path}. Please upload the file.\")\n",
        "\n",
        "\n",
        "if 'df' in locals():\n",
        "    df['risk_level'] = df['labels'].apply(lambda x: x.get('risk_level'))\n",
        "    df = df[['text', 'risk_level']]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    print(\"\\nAnonymizing and cleaning text data (using advanced NER method)...\")\n",
        "    df['processed_text'] = df['text'].apply(anonymize_and_clean_for_bert)\n",
        "\n",
        "    possible_labels = df.risk_level.unique()\n",
        "    label_dict = {label: i for i, label in enumerate(possible_labels)}\n",
        "    id_to_label = {i: label for label, i in label_dict.items()}\n",
        "\n",
        "    print(f\"\\nLabel mapping created: {label_dict}\")\n",
        "\n",
        "    df['label_id'] = df['risk_level'].replace(label_dict)\n",
        "\n",
        "    X = df['processed_text'].tolist()\n",
        "    y = df['label_id'].tolist()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(f\"\\nData split into {len(X_train)} training samples and {len(X_test)} testing samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlD9XEinVkwe"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Tokenizing training and testing data...\")\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "print(\"Tokenization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LswkdbBMVsM_"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        ")).shuffle(len(X_train)).batch(16)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        ")).batch(16)\n",
        "\n",
        "print(\"TensorFlow Datasets created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3huF4FwVunF"
      },
      "outputs": [],
      "source": [
        "print(\"Loading pre-trained DistilBERT model...\")\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(possible_labels),\n",
        "    use_safetensors=False\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ao4CoepWaMl"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "print(\"Starting model fine-tuning...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    validation_data=test_dataset\n",
        ")\n",
        "\n",
        "model.save_pretrained(\"./FineTunedBurt_model\")\n",
        "print(\"Model fine-tuning complete and saved to ./FineTunedBurt_model.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p FineTunedBurt_model\n",
        "!mv tf_model.h5 FineTunedBurt_model/\n",
        "!mv config.json FineTunedBurt_model/"
      ],
      "metadata": {
        "id": "w2fYLXq-oZdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb106fc0"
      },
      "outputs": [],
      "source": [
        "print(\"Loading the saved model...\")\n",
        "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    \"./FineTunedBurt_model\",\n",
        "    local_files_only=True,\n",
        "    num_labels=len(possible_labels)\n",
        ")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "new_sentence_to_predict = \"This is a test sentence for prediction.\"\n",
        "\n",
        "cleaned_sentence_to_predict = anonymize_and_clean_for_bert(new_sentence_to_predict)\n",
        "\n",
        "inputs_to_predict = tokenizer(cleaned_sentence_to_predict, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "outputs_predict = loaded_model(inputs_to_predict)\n",
        "logits_predict = outputs_predict.logits\n",
        "\n",
        "predicted_id_predict = tf.argmax(logits_predict, axis=-1).numpy()[0]\n",
        "\n",
        "predicted_label_predict = id_to_label[predicted_id_predict]\n",
        "\n",
        "print(f\"\\nOriginal Sentence: {new_sentence_to_predict}\")\n",
        "print(f\"Cleaned Sentence: {cleaned_sentence_to_predict}\")\n",
        "print(f\"Predicted Risk Level: {predicted_label_predict}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjeH_uKOVwTi"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating model on the test set...\")\n",
        "test_predictions = model.predict(test_dataset)\n",
        "logits = test_predictions.logits\n",
        "\n",
        "predicted_ids = tf.argmax(logits, axis=-1).numpy()\n",
        "\n",
        "print(\"\\n--- Model Evaluation Results ---\")\n",
        "accuracy = accuracy_score(y_test, predicted_ids)\n",
        "print(f\"Overall Model Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "print(\"Detailed Classification Report:\")\n",
        "print(classification_report(y_test, predicted_ids, target_names=label_dict.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GAQYaLEWWqP"
      },
      "outputs": [],
      "source": [
        "new_sentence = \"He is threatening me again and following my car. His name is Yash . His Pan is GTUDS3456T His number is 9876543210 and email is yash@email.com. I'm scared he has a weapon.\"\n",
        "print(f\"Original Sentence:\\n{new_sentence}\\n\")\n",
        "\n",
        "cleaned_sentence = anonymize_and_clean_for_bert(new_sentence)\n",
        "print(f\"Cleaned Sentence:\\n{cleaned_sentence}\\n\")\n",
        "\n",
        "inputs = tokenizer(cleaned_sentence, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "\n",
        "outputs = loaded_model(inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "predicted_id = tf.argmax(logits, axis=-1).numpy()[0]\n",
        "\n",
        "predicted_label = id_to_label[predicted_id]\n",
        "\n",
        "print(f\"Predicted Risk Level: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ9R-5Fbo19y"
      },
      "outputs": [],
      "source": [
        "print(\"Testing the anonymize_and_clean_for_bert function with various inputs:\")\n",
        "\n",
        "test_cases = [\n",
        "    \"My name is John Doe and my email is john.doe@example.com. My phone number is 123-456-7890.\",\n",
        "    \"Call me at +91 9876543210 or email me at test@test.org. My license plate is AB12CD3456.\",\n",
        "    \"The meeting is at 123 Main Street, Apartment 4B.\",\n",
        "    \"My Aadhaar number is 1234 5678 9012.\",\n",
        "    \"This sentence has no PII.\",\n",
        "    \"Mixed case and extra spaces:  My NAME is Jane Smith.  Email: Jane.Smith@EXAMPLE.ORG   Phone: (999) 888-7777 .\",\n",
        "    \"Multiple phone numbers: 111-222-3333 and 444.555.6666.\",\n",
        "    \"Address with different formats: House No. 10, Gandhi Nagar and C-5, Vihar.\",\n",
        "    \"Sentence with a person name and location: [PERSON] is in [GPE].\",\n",
        "    \"Sentence with a person name, location and organization: [PERSON] from [ORG] visited [LOC].\",\n",
        "    \"he is threatening me again, his name is Aditya and he lives in Chandigarh and his car number is MP25CE3456 and his phone number is [PHONE]\",\n",
        "    \"Bring your aadhar card with you to Andheri tomorrow.\"\n",
        "]\n",
        "\n",
        "for i, test_text in enumerate(test_cases):\n",
        "    anonymized_cleaned_text = anonymize_and_clean_for_bert(test_text)\n",
        "    print(f\"\\n--- Test Case {i+1} ---\")\n",
        "    print(f\"Original: {test_text}\")\n",
        "    print(f\"Processed: {anonymized_cleaned_text}\")\n",
        "    new_sentence = \"He is threatening me again and following my car. His name is Aditya. His number is 9876543210 and email is aditya@email.com. I'm scared he has a weapon.\"\n",
        "\n",
        "    inputs = tokenizer(anonymized_cleaned_text, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = loaded_model(inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_id = tf.argmax(logits, axis=-1).numpy()[0]\n",
        "    predicted_label = id_to_label[predicted_id]\n",
        "    print(f\"Predicted Risk Level: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gnwFOkfjHqk"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "history = []\n",
        "\n",
        "risk_colors = {\n",
        "    \"Low\": \"#4CAF50\",\n",
        "    \"Medium\": \"#FFC107\",\n",
        "    \"High-Urgency\": \"#FF7043\",\n",
        "    \"Immediate-Threat\": \"#F44336\"\n",
        "}\n",
        "\n",
        "def predict_risk_level(user_text):\n",
        "    if not user_text.strip():\n",
        "        return {\"Error\": \"Please enter some text.\"}\n",
        "\n",
        "    processed_text = anonymize_and_clean_for_bert(user_text)\n",
        "    inputs = tokenizer(processed_text, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
        "    outputs = loaded_model(inputs)\n",
        "    probs = tf.nn.softmax(outputs.logits, axis=-1).numpy()[0]\n",
        "    pred_id = np.argmax(probs)\n",
        "    pred_label = id_to_label[pred_id]\n",
        "    confidence = float(np.max(probs))\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "\n",
        "    history.append({\n",
        "        \"Time\": timestamp,\n",
        "        \"Risk\": pred_label,\n",
        "        \"Confidence\": f\"{confidence:.2f}\",\n",
        "        \"Text\": processed_text\n",
        "    })\n",
        "\n",
        "    color = risk_colors.get(pred_label, \"#9E9E9E\")\n",
        "    bar_html = f\"\"\"\n",
        "    <div style='width:100%; background:#e0e0e0; border-radius:8px;'>\n",
        "        <div style='width:{confidence*100:.1f}%; background:{color}; padding:4px; border-radius:8px; text-align:center; color:white;'>\n",
        "            {confidence*100:.1f}%\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_result = f\"\"\"\n",
        "    <div style='font-family:monospace;'>\n",
        "        <p><b>Predicted Risk:</b> <span style='color:{color}; font-weight:600;'>{pred_label}</span></p>\n",
        "        <p><b>Confidence:</b></p>{bar_html}\n",
        "        <hr>\n",
        "        <p><b>Anonymized Input:</b> {processed_text}</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    hist_table = \"\\n\".join([f\"üïí {h['Time']} | {h['Risk']} ({h['Confidence']}) ‚Üí {h['Text']}\" for h in history[-5:]])\n",
        "    return html_result, hist_table\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"<h2 style='text-align:center'>üõ°Ô∏è Domestic Safety Risk Level Classifier</h2>\")\n",
        "    gr.Markdown(\"Predicts message **risk level**  All inputs are automatically anonymized.\")\n",
        "    txt = gr.Textbox(lines=4, placeholder=\"Type your message here...\")\n",
        "    res = gr.HTML()\n",
        "    hist = gr.Textbox(label=\"Recent Predictions (last 5)\", interactive=False)\n",
        "    btn = gr.Button(\"üîç Analyze\", variant=\"primary\")\n",
        "    btn.click(predict_risk_level, inputs=txt, outputs=[res, hist])\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}